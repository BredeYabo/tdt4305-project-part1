\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes} \usepackage{enumitem} \usepackage{hyperref}
\hypersetup{colorlinks = true}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{caption}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE NTNU}\\[1.5cm] % Name of your university/college
\textsc{\Large TDT4225 Store, distribuerte datamengder}\\[0.5cm] % Major heading such as course name
\textsc{\large Assignment \#4}\\[0.5cm] % Minor heading such as course title

\HRule \\[0.4cm]
{ \huge \bfseries Ã˜ving 4}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 

\emph{Medvirkende:}\\
Brede Yabo \textsc{Kristensen}\\ % Your name

~

\emph{} \\


{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

\includegraphics[width=55mm,scale=1.0]{logo_ntnu_bokm.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------
\title{}
\vfill % Fill the rest of the page with whitespace


\end{titlepage}

\section{RDD API Tasks}

The primary focus of using the spark functions is to only transform the RDD before the final execution, which is saving it a file. This reduces computational requirement for the program. This means that we should avoid using parrallelize() or take() in the middle of the program at all cost.

\subsection{Task 1}

The files we are reading are rows with columns seperated by , .

I first split each column using , as a deliminator. This is done by using the map() function as it applies a function to all of the rows. The result is yet another RDD. When splitting the rows we get an array for each row, we then want the fourth column as it is the genre column. We could of used flatmap(), but we want to an rdd that has the same number of rows.

We then use the function distinct() to remove all duplicate from the rdd. Finally we count the number of rows using the count() function, which returns  number.


\subsection{Task 2}

We start the same way as last time, only now we get the fourth column from the artist.csv file (year). We also make sure that the rows are converted to integers as we will compare them later. We then reduce the rdd using the reduce() function. We want to return the oldest artist and do this by comparing all the rows.

Finally we print the oldest artist birth date.

\subsection{Task 3}

Like before, we split the lines but now as a tupple with the integer 1, used for counting later. We then use reduceByKey() which executes the function for each rows that have the same key, in our case, we count each time and add it to the counter in the tupple. This leaves us with the number of artist for each country.

The task specifies that we sort the total number of artist for each country descending and artist alphabetically for countries with the same number of artists. One thing to note when using hadoop/spark is that the data is split into different partitions. So we can't call sortByKey two times for each key and value, because this function shuffles the last order, therefor not preserving the previous order. We begin by sorting by countries as normal, then we map the tupples into ((count,country),country) so that we keep the country ordering when inserting it back to the normal tupple (country, count). We also specify in this sort that we want to sort only count and nothing else, or else it will also sort countries alphabetically Descending, which is not what we want.

The result is finally outputted to result_3 with coalesce, shuffle turned off to preserve the order. Read more about why coalesce is chosen in it's subsection.

\subsection{Task 4}

We begin by mapping out artist_id column and keeping a number 1 in the tupple from the album file. The reason for doing this is because we know that there are duplicates in artist_id because some artist have several albums. We then reduceByKey() which executes the function inside whenever we have the same artist_id. The function basically counts each time for each same artist_id and adds it to the last element in the tupple. 

We then sort artist_id descending and do the same for count only now we use the same technique as we did with task 3 to preserve the previous sort order.

\subsection{Task 5}

In this task we create a tupple with out of two colums, (genre, number_of_sales).

\subsection{Coalesce vs repartition}

Coalesce is being used with shuffle being false. This is because it preserves the ordering when sorting the data into one partition. Repartition shuffles the data when it combines it into one partition. Repartition calls Coalesce with shuffle as true, which will not preserve the order.

\end{document}


